lesson_10:
  lesson_title: "Building a Neural Network from Scratch"
  lesson_description: 
    In this final lesson, you will learn how to build a simple neural network from scratch using Python. 
    This network will be used for binary classification, and you will implement all key steps like forward propagation, 
    backward propagation, and parameter updates.

  steps:
    - step_title: "1. Initialize Parameters"
      description: 
        We begin by initializing the parameters (weights and biases) for the neural network. 
        The weights are initialized randomly, and biases are initialized to zero.
      code: |
        def initialize_parameters(input_size, hidden_size, output_size):
            W1 = np.random.randn(hidden_size, input_size) * 0.01
            b1 = np.zeros((hidden_size, 1))
            W2 = np.random.randn(output_size, hidden_size) * 0.01
            b2 = np.zeros((output_size, 1))
            parameters = {"W1": W1, "b1": b1, "W2": W2, "b2": b2}
            return parameters

    - step_title: "2. Forward Propagation"
      description: 
        In forward propagation, we calculate the output of the network based on the current parameters. 
        The activation function used here is the sigmoid function.
      code: 
        def sigmoid(Z):
            return 1 / (1 + np.exp(-Z))

        def forward_propagation(X, parameters):
            W1 = parameters["W1"]
            b1 = parameters["b1"]
            W2 = parameters["W2"]
            b2 = parameters["b2"]
            Z1 = np.dot(W1, X) + b1
            A1 = sigmoid(Z1)
            Z2 = np.dot(W2, A1) + b2
            A2 = sigmoid(Z2)
            cache = {"A1": A1, "A2": A2}
            return A2, cache

    - step_title: "3. Cost Function"
      description: 
        To evaluate the performance of our network, we use a cost function (binary cross-entropy) 
        to measure the error in our predictions.
      code: 
        def compute_cost(A2, Y):
            m = Y.shape[1]
            cost = -np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2)) / m
            return cost

    - step_title: "4. Backward Propagation"
      description: 
        Backward propagation helps us compute the gradients of the cost function with respect to the parameters. 
        Using these gradients, we can update the parameters to minimize the cost.
      code: 
        def sigmoid_derivative(A):
            return A * (1 - A)

        def backward_propagation(X, Y, parameters, cache):
            m = X.shape[1]
            A1 = cache["A1"]
            A2 = cache["A2"]
            dZ2 = A2 - Y
            dW2 = np.dot(dZ2, A1.T) / m
            db2 = np.sum(dZ2, axis=1, keepdims=True) / m
            dZ1 = np.dot(parameters["W2"].T, dZ2) * sigmoid_derivative(A1)
            dW1 = np.dot(dZ1, X.T) / m
            db1 = np.sum(dZ1, axis=1, keepdims=True) / m
            gradients = {"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2}
            return gradients

    - step_title: "5. Update Parameters"
      description: 
        Once we have the gradients, we use them to update the parameters using gradient descent.
      code: 
        def update_parameters(parameters, gradients, learning_rate):
            W1 = parameters["W1"] - learning_rate * gradients["dW1"]
            b1 = parameters["b1"] - learning_rate * gradients["db1"]
            W2 = parameters["W2"] - learning_rate * gradients["dW2"]
            b2 = parameters["b2"] - learning_rate * gradients["db2"]
            parameters = {"W1": W1, "b1": b1, "W2": W2, "b2": b2}
            return parameters

    - step_title: "6. Train the Model"
      description: 
        In the training loop, we will repeatedly perform forward and backward propagation and update the parameters.
      code: 
        def train_neural_network(X, Y, hidden_size, learning_rate, num_iterations):
            input_size = X.shape[0]
            output_size = Y.shape[0]
            parameters = initialize_parameters(input_size, hidden_size, output_size)
            for i in range(num_iterations):
                A2, cache = forward_propagation(X, parameters)
                cost = compute_cost(A2, Y)
                gradients = backward_propagation(X, Y, parameters, cache)
                parameters = update_parameters(parameters, gradients, learning_rate)
                if i % 1000 == 0:
                    print(f"Cost after iteration {i}: {cost}")
            return parameters

    - step_title: "7. Evaluate the Model"
      description: 
        Finally, we evaluate the model by making predictions and comparing the output to a threshold (0.5) 
        to classify the data.
      code: 
        def predict(X, parameters):
            A2, _ = forward_propagation(X, parameters)
            predictions = (A2 > 0.5).astype(int)
            return predictions

  conclusion: 
    By building a neural network from scratch, youâ€™ve learned key concepts like parameter initialization, 
    forward propagation, cost calculation, backward propagation, and gradient descent. 
    This gives you a strong foundation before working with advanced libraries such as TensorFlow or PyTorch.

  next_steps:
    - Try enhancing the network (e.g., adding more layers).
    - Experiment with other activation functions.
    - Learn about advanced optimization techniques such as Adam.
